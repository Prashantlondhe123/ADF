  Pipeline is a logical grouping of activities
that perform a unit of work. You define work
 
 performed by ADF as a pipeline of operations.
Data Factory might have one or more pipelines.

 Pipeline can be run manually, or run using a trigger.
Activities in a pipeline can be chained together to operate 
sequentially, or they can operate independently in parallel.
For example, you might use a copy activity to 
transform data from a source dataset to
 a destination dataset. 
   You could include activities that transform data as it is
transferred, or you might combine data from
multiple sources together.
